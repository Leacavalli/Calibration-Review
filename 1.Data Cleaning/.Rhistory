filter(! MUT %in% Rare_mutation)|>
mutate(AF_new= ifelse(AF1 > 0.5, 1-AF1, AF1))
# How many unique mutations ?
length(unique(iSNV_SNP_single_rare$MUT))
parse_number(unique(iSNV_SNP_single_rare$MUT))
# How recurrent in each iSNV ?
iSNV_SNP_single_rare |>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)
#  AF
summary(iSNV_SNP_single_rare$AF_new)
summary(iSNV_SNP_single_common$AF_new)
length(unique(iSNV_SNP_single_rare$MUT))
iSNV_SNP_single_rare
nrow(iSNV_SNP_single_rare)
iSNV_SNP_single_rare |>
group_by(MUT)|>
summary(N=n())|>
arrange(-N)
iSNV_SNP_single_rare |>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)
Analysis_VCF_iSNVs|>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)
Analysis_VCF_iSNVs|>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)|>
filter(MUT== "C10818T")
Analysis_VCF_iSNVs|> filter(MUT== "C10818T")
Rare_mutation <- Analysis_VCF |>
filter(AF_V3 >= 0.95)|>
filter(MUT %in% (iSNV_SNP_df_under10_genclose |> pull(MUT)))|>
group_by(MUT)|>
summarise(N=n())|>
filter(N < 5)|>
pull(MUT)
## Filter
iSNV_SNP_single_rare <- iSNV_SNP_df_under10_genclose |>
filter(MUT %in% Rare_mutation)|>
mutate(AF_new= ifelse(AF1 > 0.5, 1-AF1, AF1))
iSNV_SNP_single_common<- iSNV_SNP_df_under10_genclose |>
filter(! MUT %in% Rare_mutation)|>
mutate(AF_new= ifelse(AF1 > 0.5, 1-AF1, AF1))
# How many unique mutations ?
length(unique(iSNV_SNP_single_rare$MUT))
iSNV_SNP_single_rare |>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)
Analysis_VCF_iSNVs|>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)|>
filter(MUT== "C10818T")
parse_number(unique(iSNV_SNP_single_rare$MUT))
# How recurrent in each iSNV ?
iSNV_SNP_single_rare |>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)
nrow(iSNV_SNP_single_rare)
Rare_mutation <- Analysis_VCF |>
filter(AF_V3 >= 0.95)|>
filter(MUT %in% (iSNV_SNP_df_under10_genclose |> pull(MUT)))|>
group_by(MUT)|>
summarise(N=n())|>
filter(N < 5)|>
pull(MUT)
## Filter
iSNV_SNP_single_rare <- iSNV_SNP_df_under10_genclose |>
filter(MUT %in% Rare_mutation)|>
mutate(AF_new= ifelse(AF1 > 0.5, 1-AF1, AF1))
iSNV_SNP_single_common<- iSNV_SNP_df_under10_genclose |>
filter(! MUT %in% Rare_mutation)|>
mutate(AF_new= ifelse(AF1 > 0.5, 1-AF1, AF1))
# How many unique mutations ?
length(unique(iSNV_SNP_single_rare$MUT))
# How many sample pairs
nrow(iSNV_SNP_single_rare)
iSNV_SNP_single_rare |>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)
Analysis_VCF|>
group_by(MUT)|>
summarise(N=n())|>
arrange(-N)|>
filter(MUT== "C10818T")
Analysis_VCF|>filter(MUT== "C10818T")
unique(iSNV_SNP_single_rare$MUT)
Analysis_VCF|>filter( Sample=="bu6134")
Analysis_VCF_metadata|>filter( Sample=="bu6134") |> select(Collection.date, Library.plate)
Analysis_VCF_metadata|> filter(Library.plate == "artic-batch62")
Analysis_VCF_metadata|> filter(Library.plate == "artic-batch62") |> pull(VoC)
table(Analysis_VCF_metadata|> filter(Library.plate == "artic-batch62") |> pull(VoC))
names(superinfection)
superinfection
unique_pairs |>
ungroup()|>
filter(sample1 == names(superinfection)[1])
unique_pairs |>
ungroup()|>
filter(sample1 == names(superinfection)[2])
# List of unique mutations
summary(iSNV_SNP_single_rare$AF1)
## Load and prepare data
# Clean environment
rm( list = ls() )
# load necessary libraries
library(tidyverse)
library(dplyr)
# load VCF and metadata data
setwd("~/GitHub/Sars-cov-2-Intrahost-Variation/2.Data_Cleaning")
Analysis_VCF <- read.csv("Analysis_VCF.csv") |> select(-X)
Analysis_VCF_iSNVs <- Analysis_VCF |> filter(AF_V3<0.95)
# load metadata
Analysis_metadata <- read.csv("Analysis_metadata.csv")|>
filter(Sample %in% Analysis_VCF$Sample)
# join
Analysis_VCF_metadata <- left_join(Analysis_VCF, Analysis_metadata)
# load all Shared iSNVs
setwd("~/GitHub/Sars-cov-2-Intrahost-Variation/3. Data Analysis/3.2. Tranmission")
All_shared_iSNV_df <- read.csv("shared_mutations_df.csv")
# How many sample pairs
nrow(All_shared_iSNV_df)
# How many share at least 1 iSNVs
nrow(All_shared_iSNV_df |> filter(shared_count!=0))
nrow(All_shared_iSNV_df |> filter(shared_count!=0))/nrow(All_shared_iSNV_df)
# Table 1
Table_S1 <- All_shared_iSNV_df |>
group_by(shared_count) |>
summarise(N= n()) |>
mutate(P=  paste(" (", round(100*N/sum(N),2), "%)", sep=""))
Table_S1
# PLOT PHYLOGENETIC TREE
# Load phylogeny
library(ape)
setwd("~/GitHub/Sars-cov-2-Intrahost-Variation/0.Raw_data")
timetree <- read.tree("IQtree.timetree.nwk")
timetree <- keep.tip(timetree, Analysis_metadata$Sample)
# Prepare colours
library(ggsci)
jco_palette <- pal_jco("default")(4)
names(jco_palette) <- c("Delta","BA.1","BA.2","Reference")
# Add metadata to phylo
library(treeio)
timetree_data <- full_join(timetree, Analysis_metadata |>
select(Sample, VoC)|>
rename("label"="Sample"), by="label") |>
mutate(VoC=factor(VoC, levels= c("Alpha","Iota","Delta","BA.1","BA.2","Omicron","Unassigned","Reference")))
# Plot phylogeny
library(ggtree)
phylo_timetree <- ggtree(timetree_data, ladderize = T, lwd=0.1) +
geom_tippoint(aes(colour= VoC),size=1)+
scale_color_manual(values = jco_palette)+
theme(legend.position = "none")
# PLOT SHARED ISNV LINKS
##  tip data
d = fortify(timetree)
d = subset(d, isTip)
tip_order <- with(d, label[order(y, decreasing=F)])
nodes <- tibble(nodes = tip_order, pos = 1:length(tip_order))
## Your data manipulation code
plotdf <- All_shared_iSNV_df %>%
filter(shared_count > 0) %>%
select(sample1, sample2, shared_count) %>%
mutate(shared_count_cat = ifelse(shared_count >= 5, ">5", shared_count)) %>%
rename("from" = "sample1", "to" = "sample2")
## Create the graph object
gr <- igraph::graph_from_data_frame(plotdf, vertices = nodes)
## Generate the plot
library(ggraph)
link <- ggraph(gr,
layout = 'manual',
y = rep(0, length(nodes$pos)),
x = nodes$pos) +
geom_edge_arc(alpha = 0.5, fold = TRUE) +
scale_x_continuous(breaks = 1:length(tip_order), labels = tip_order) +
coord_flip() +
facet_grid(. ~ factor(shared_count_cat, levels = c( "1", "2", "3", "4",">5")), scales = "free", space = "free") +
theme(strip.text.x = element_text(size = 10),
panel.background = element_blank())
# COMBINE PLOTS
library(ggpubr)
ggarrange(phylo_timetree+ theme(plot.margin = margin(50, 5, 28, 5)),
link,
ncol = 2, nrow = 1,
widths = c(2,6))
# Add collection date information
Shared_iSNVs_sample1 <- left_join(All_shared_iSNV_df |> select(sample1) |> rename("Sample" = "sample1"), Analysis_metadata |> select(Sample, Aliquot.plate, Library.plate, Collection.date, lineage, VoC)) |> rename("sample" = "Sample")
colnames(Shared_iSNVs_sample1) <- paste0(colnames(Shared_iSNVs_sample1), "1")
Shared_iSNVs_sample2 <- left_join(All_shared_iSNV_df |> select(sample2) |> rename("Sample" = "sample2"), Analysis_metadata |> select(Sample, Aliquot.plate, Library.plate, Collection.date, lineage, VoC))|> rename("sample" = "Sample")
colnames(Shared_iSNVs_sample2) <- paste0(colnames(Shared_iSNVs_sample2), "2")
Shared_iSNVs_sample_df <- cbind(Shared_iSNVs_sample1, Shared_iSNVs_sample2,
All_shared_iSNV_df |> select(shared_count))|>
mutate(Collection.date1=as.Date(Collection.date1),
Collection.date2=as.Date(Collection.date2),
diff_Collection.date= abs(difftime(Collection.date1, Collection.date2, units = "days")))
# Add cophenetic distances
library(ape)
library(ggsci)
setwd("~/GitHub/Sars-cov-2-Intrahost-Variation/0.Raw_data")
cleantree <- read.tree("clean_tree.tre")
library(reshape2)
cophenetic_dist <- cophenetic.phylo(cleantree)
cophen_df_melt <- melt(cophenetic_dist) |>
rename("sample1"="Var1", "sample2"="Var2", "cophenetic_dist"="value")
Shared_iSNVs_cophdist_df <- left_join(Shared_iSNVs_sample_df, cophen_df_melt)|>
mutate(shared_count_cat= ifelse(shared_count ==0 , "No shared iSNVs", "Shared iSNVs"))
# SUBSET TRANSMISSION PAIRS:
# criteria: shared iSNVs, small cophenetic distance, collection dates <10 days appart
Shared_iSNVs_transmission_df <- Shared_iSNVs_cophdist_df |>
filter(VoC1 ==VoC2)|>
mutate(VoC1=factor(VoC1, levels= c("Delta","BA.1","BA.2")))|>
group_by(VoC1)|>
mutate(threshold = quantile(cophenetic_dist, 0.05, na.rm = TRUE))|>
ungroup()|>
filter(cophenetic_dist < threshold, diff_Collection.date <= 10) |>
filter(shared_count > 0)
# PLOT SHARED ISNV LINKS
plotdf_V2 <- Shared_iSNVs_transmission_df %>%
select(sample1, sample2, shared_count) %>%
rename("from" = "sample1", "to" = "sample2")
## Create the graph object
gr_V2 <- igraph::graph_from_data_frame(plotdf_V2, vertices = nodes)
## Generate the plot
library(ggraph)
library(cartography)
link_V2 <- ggraph(gr_V2,
layout = 'manual',
y = rep(0, length(nodes$pos)),
x = nodes$pos) +
geom_edge_arc(fold = TRUE, aes(color = as.factor(shared_count), width=as.factor(shared_count)))+
scale_edge_color_manual(name="Number of Shared iSNVs",
values=carto.pal(pal1 = "pastel.pal" ,n1 = 15),
guide = guide_legend(override.aes = list(edge_width   = 5))) +
scale_edge_width_manual(values=seq(from=1, to=4, by=0.25), guide="none")+
coord_flip() +
theme(strip.text.x = element_text(size = 10),
panel.background = element_blank())
# COMBINE PLOTS
library(ggpubr)
ggarrange(phylo_timetree+ theme(plot.margin = margin(30, 5, 30, 5)),
link_V2,
ncol = 2, nrow = 1,
widths = c(2,4))
# Count number of pairs per VoC
Shared_iSNVs_transmission_df |>
group_by(VoC1) |>
summarise(N=n())
Shared_iSNVs_transmission_df
nrow(Shared_iSNVs_transmission_df)
# Add collection date information
Shared_iSNVs_sample1 <- left_join(All_shared_iSNV_df |> select(sample1) |> rename("Sample" = "sample1"), Analysis_metadata |> select(Sample, Aliquot.plate, Library.plate, Collection.date, AF, lineage, VoC)) |> rename("sample" = "Sample")
# Add collection date information
Shared_iSNVs_sample1 <- left_join(All_shared_iSNV_df |> select(sample1) |> rename("Sample" = "sample1"), Analysis_metadata |> select(Sample, Aliquot.plate, Library.plate, Collection.date, AF_V3, lineage, VoC)) |> rename("sample" = "Sample")
# Add collection date information
Shared_iSNVs_sample1 <- left_join(All_shared_iSNV_df |> select(sample1) |> rename("Sample" = "sample1"), Analysis_VCF_metadata |> select(Sample, Aliquot.plate, Library.plate, Collection.date, AF_V3, lineage, VoC)) |> rename("sample" = "Sample")
nrow(Shared_iSNVs_transmission_df)
All_shared_iSNV_df
colnames(Shared_iSNVs_sample1) <- paste0(colnames(Shared_iSNVs_sample1), "1")
All_shared_iSNV_df
Shared_iSNVs_sample2 <- left_join(All_shared_iSNV_df |> select(sample2) |> rename("Sample" = "sample2"), Analysis_VCF_metadata |> select(Sample, Aliquot.plate, Library.plate, Collection.date, AF_V3, lineage, VoC))|> rename("sample" = "Sample")
colnames(Shared_iSNVs_sample2) <- paste0(colnames(Shared_iSNVs_sample2), "2")
Shared_iSNVs_sample_df <- cbind(Shared_iSNVs_sample1, Shared_iSNVs_sample2,
All_shared_iSNV_df |> select(shared_count))|>
mutate(Collection.date1=as.Date(Collection.date1),
Collection.date2=as.Date(Collection.date2),
diff_Collection.date= abs(difftime(Collection.date1, Collection.date2, units = "days")))
knitr::opts_chunk$set(echo = TRUE)
w <- 3/4*(beta(17,36)/beta(2,8))/(3/4*(beta(17,36)/beta(2,8)) + 1/4*(beta(23,30)/beta(8,2)))
w <- 3/4*(beta(17,36)/beta(2,8))/(3/4*(beta(17,36)/beta(2,8)) + 1/4*(beta(23,30)/beta(8,2)))
#w
mixture_posterior_dist2 <- w*dbeta(x1, 17, 36) + (1-w)*dbeta(x1, 23, 30)
w <- 3/4*(beta(17,36)/beta(2,8))/(3/4*(beta(17,36)/beta(2,8)) + 1/4*(beta(23,30)/beta(8,2)))
#w
mixture_posterior_dist2 <- w*dbeta(x1, 17, 36) + (1-w)*dbeta(x1, 23, 30)
x1 <- seq(0, 1, 0.001)
w <- 3/4*(beta(17,36)/beta(2,8))/(3/4*(beta(17,36)/beta(2,8)) + 1/4*(beta(23,30)/beta(8,2)))
#w
mixture_posterior_dist2 <- w*dbeta(x1, 17, 36) + (1-w)*dbeta(x1, 23, 30)
# Sequence directly from the distribution
z_vec <- vector()
for (i in seq(1:1e6)) {
if(rbinom(1,1,w) == 1){
z_vec[i] <- rbeta(1, 17, 36)
} else {
z_vec[i] <- rbeta(1, 23, 30)
}
}
for (i in seq(1:1e6)) {
if(rbinom(1,1,w) == 1){
z_vec[i] <- rbeta(1, 17, 36)
} else {
z_vec[i] <- rbeta(1, 23, 30)
}
}
#z_vec
round(quantile(z_vec, probs = c(0.025,0.975)),3)
round(quantile(z_vec, probs = c(0.025,0.975)),3)
ggplot() +
geom_density(aes(x = z_vec))+
geom_vline(xintercept = 0.314, color = "darkred") +
geom_vline(xintercept = lower_bound, color = "darkgrey") +
geom_vline(xintercept = upper_bound, color = "darkgrey") +
theme_bw()
library(tidyverse)
ggplot() +
geom_density(aes(x = z_vec))+
geom_vline(xintercept = 0.314, color = "darkred") +
geom_vline(xintercept = lower_bound, color = "darkgrey") +
geom_vline(xintercept = upper_bound, color = "darkgrey") +
theme_bw()
ggplot() +
geom_density(aes(x = z_vec))+
geom_vline(xintercept = 0.314, color = "darkred") +
#  geom_vline(xintercept = lower_bound, color = "darkgrey") +
#  geom_vline(xintercept = upper_bound, color = "darkgrey") +
theme_bw()
96*2
library(fastmap)
remove.packages("fastmatch")
install.packages("fastmap")
library(ggraph)
library(khorma)
library(khroma)
colours("light")
colors("light")
colour("light")
colour("light")(7)
2+48+20
8*11
2/6
1/6
55/3
library(xlsx)
setwd("~/GitHub/GBS-Global-NFDS/0.Data/0.1.Publicly available data")
Netherlands_metadata <- read.xlsx("Netherlands_Sanger_metadata.xlsx", sheetName ="Isolate characteristics")
CDC_metadata <- read.xlsx("CDC_metadata.xlsx", sheetName ="Sheet1")
CDC_metadata_EOD_LOD <- CDC_metadata |> filter(! is.na(Biosample.Accession)) |> filter(Age.group %in% c("EOD", "LOD"))
80*4
80*5
73.7+19.6+5.5
21.7+35.6+42.7
20.3+40.1+45.3
1796888-1796391
1682994-1682497
20255-19755
25*7
20*3*7
30*3*7
630+400
1400/3/7
410/3/7
19.52381+50
25*7
1600/3
1600/3/7
40*3*7
840+410
1250/3
1250/3/7
68169270/1833
37190*498
1834+498
1334+498
library(readxl) # Reading Excel graphs
# library(ggstatsplot) # Plotting results
library(ggpubr) # Combining plots
#### Load data and format #####
# Clean environment
rm( list = ls() )
# load necessary libraries
library(tidyverse)
# Load data
setwd("~/GitHub/Calibration-Review/1.Data Cleaning")
clean_data <- read.csv("Clean_data.csv")
# tb1
tb1 <-
clean_data |>
separate_rows(Calibration.algorithm.Family, sep = "; ")|>
select(Covidence.ID, Full.title, Calibration.algorithm.Family, Model.type, Stochasticity.in.model)
### General data wrangling ####
# Import
# tb1 <- read_excel("data/Objective2_Table1a.xlsx")
head(tb1)
# Subset to include only top calibration methods with at least 20 models in each
tb11 <- tb1[tb1$Calibration.algorithm.Family%in%
c("Approximate Bayesian Computation (ABC)",
"Markov Chain Monte Carlo",
"Least squares estimation",
"Maximum likelihood estimation") & tb1$Model.type %in% c("Compartmental", "Individual-based"), ]
tb11
## Calibration algorithm, factor
tb11$Calibration.algorithm.Family <- as.factor(tb11$Calibration.algorithm.Family)
levels(tb11$Calibration.algorithm.Family)
## Model structure
tb11$Model.type <- as.factor(tb11$Model.type)
levels(tb11$Model.type)
## Model stochasticity
tb11$Stochasticity.in.model <- as.factor(tb11$Stochasticity.in.model)
levels(tb11$Stochasticity.in.model)
# View contingency table
table(tb11$Calibration.algorithm.Family, tb11$Model.type)
function (x, y = NULL, workspace = 2e+05, hybrid = FALSE, hybridPars = c(expect = 5,
percent = 80, Emin = 1), control = list(), or = 1, alternative = "two.sided",
conf.int = TRUE, conf.level = 0.95, simulate.p.value = FALSE,
B = 2000)
# Run Fisher's test (because expected values are less than 5)
dat1 <- table(tb11$Calibration.algorithm.Family, tb11$Model.type)
test1 <- fisher.test(dat1)
# Run Fisher's test (because expected values are less than 5)
dat1 <- table(tb11$Calibration.algorithm.Family, tb11$Model.type)
test1 <- fisher.test(dat1)
test1$data.name
# Fisher's exact test plot
# create dataframe from contingency table
x <- c()
for (row in rownames(dat1)) {
for (col in colnames(dat1)) {
x <- rbind(x, matrix(rep(c(row, col), dat1[row, col]), ncol = 2, byrow = TRUE))
}
}
df <- as.data.frame(x)
colnames(df) <- c("Calibration method", "Model structure")
df
# Run Chi-sq test (ref: https://statsandr.com/blog/chi-square-test-of-independence-in-r/)
# table(tb1$Calibration.algorithm.Family, tb1$Model.type)
test <- chisq.test(table(tb1$Calibration.algorithm.Family, tb1$Model.type))
# Run Chi-sq test (ref: https://statsandr.com/blog/chi-square-test-of-independence-in-r/)
# table(tb1$Calibration.algorithm.Family, tb1$Model.type)
test <- chisq.test(table(tb1$Calibration.algorithm.Family, tb1$Model.type))test$expected
test$expected
ggbarstats(
df, `Calibration method`, `Model structure`,
results.subtitle = FALSE,
ylab = "Percentage of models",
package = "colorBlindness",
palette = "paletteMartin"
) +
theme(
axis.title = element_text(size = 16),  # Axis titles
axis.text = element_text(size = 14),   # Axis tick labels
legend.title = element_text(size = 14), # Legend title
legend.text = element_text(size = 12) )  # Legend text)
test1
# Run Fisher's test
dat2 <- table(tb12$Calibration.algorithm.Family, tb12$Stochasticity.in.model)
dat2
test2 <- chisq.test(dat2)
test2
# Subset to include only top calibration methods with at least 20 models in each
tb12 <- tb1[tb1$Calibration.algorithm.Family%in%
c("Approximate Bayesian Computation (ABC)",
"Markov Chain Monte Carlo",
"Least squares estimation",
"Maximum likelihood estimation") & tb1$Stochasticity.in.model %in% c("Deterministic model ", "Stochastic model"), ]
tb12
# Run Fisher's test
dat2 <- table(tb12$Calibration.algorithm.Family, tb12$Stochasticity.in.model)
dat2
test2 <- chisq.test(dat2)
test2
ggbarstats(
df2, `Calibration method`, `Model stochasticity`,
results.subtitle = FALSE,
ylab = "Percentage of models",
package = "colorBlindness",
palette = "paletteMartin"
)+
theme(
axis.title = element_text(size = 16),  # Axis titles
axis.text = element_text(size = 14),   # Axis tick labels
legend.title = element_text(size = 14), # Legend title
legend.text = element_text(size = 12) )  # Legend text)
library(ggstatsplot)
ggbarstats(
df2, `Calibration method`, `Model stochasticity`,
results.subtitle = FALSE,
ylab = "Percentage of models",
package = "colorBlindness",
palette = "paletteMartin"
)+
theme(
axis.title = element_text(size = 16),  # Axis titles
axis.text = element_text(size = 14),   # Axis tick labels
legend.title = element_text(size = 14), # Legend title
legend.text = element_text(size = 12) )  # Legend text)
install.packages(ggstatsplot)
install.packages("ggstatsplot")
install.packages("ggstatsplot")
library(ggstatsplot)
ggbarstats(
df2, `Calibration method`, `Model stochasticity`,
results.subtitle = FALSE,
ylab = "Percentage of models",
package = "colorBlindness",
palette = "paletteMartin"
)+
theme(
axis.title = element_text(size = 16),  # Axis titles
axis.text = element_text(size = 14),   # Axis tick labels
legend.title = element_text(size = 14), # Legend title
legend.text = element_text(size = 12) )  # Legend text)
install.packages("insight")
install.packages("insight")
library(ggstatsplot)
