---
title: "Data cleaning"
output: html_document
date: "2024-05-04"
Author: LÃ©a Cavalli
---

# Set up environment
```{r}
# Clean environment
rm( list = ls() )

# load necessary libraries
library(tidyverse)

# Load data
setwd("~/GitHub/Calibration-Review")
raw_data <- read.csv("review_396964_20240504214514.csv")
```

# Correct country inconsistencies
```{r}
# Explore country names 
raw_data |> 
  group_by(Study.setting) |>
  summarise(N=n())|>
  arrange(-N)
# install.packages("countries")
library(countries)
list_countries <- list_countries(nomenclature = "name_en")
raw_data |> 
  filter(! Study.setting %in% list_countries) |> 
  pull(Study.setting)


# Correcting some inconsistencies
raw_data_V2 <- raw_data |>
  mutate(Study.setting = ifelse(Study.setting %in% c("United States", "USA"), "United States of America", Study.setting))|>
  mutate(Study.setting = ifelse(Study.setting %in% c("eSwatini ", "Swaziland"), "Eswatini", Study.setting))|>
  mutate(Study.setting = ifelse(Study.setting %in% c("UK"), "United Kingdom", Study.setting))|>
  mutate(Study.setting = ifelse(Study.setting %in% c("Russia"), "Russian Federation", Study.setting))|>
  mutate(Study.setting = str_replace(Study.setting, ", and ", ", "))|>
  mutate(Study.setting = str_replace(Study.setting, "and ", ", "))
# still: # South Korea vs Democratic People's Republic of Korea

unique(raw_data_V2|> 
  filter(! Study.setting %in% list_countries) |> 
  pull(Study.setting))


# Outliers 
raw_data_V2|> 
  filter(Study.setting %in% c("105 LMICs", "Southeast Asia", "sub-Saharan Africa", "Not reported"))

# 3251: Study.setting = "105 LMICs"
# 1140: Study.setting = "Southeast Asia
# 1099: Study.setting = "sub-Saharan Africa"
# 994: Study.setting = "Not reported"

```



# Investigate outliers 
```{r}
# Data cleaning 
# remove those that have multiple calibration ?
# size of calibration output: commas, no commas
# Outliers

raw_data  |> 
  filter(How.many.parameters.were.calibrated. =="")
# 351 :  How.many.parameters.were.calibrated. is empty


raw_data  |> 
  filter(grepl("Other: ", Name.of.calibration.algorithm))
# 3216 : Other: Method of moments	
# 894 :  Other: Latin Hypercube Sampling	


raw_data |> 
    filter(grepl("Other", Nature.of.calibration.results))
# 845: Multiple calibration

raw_data  |> 
  filter(External.beliefs.or.evidence =="")
# 1227: External.beliefs.or.evidence is empty


raw_data  |> 
  filter(How.are.calibration.results.reported. == "Numerical: e.g. as value(s).; Calibration results are not reported. ")
# 789: How.are.calibration.results.reported. == "Numerical: e.g. as value(s).; Calibration results are not reported. "

raw_data  |> 
  filter(How.is.uncertainty.in.calibration.outputs.reported. == "Graphical: As a plot; e.g., a line with shaded areas indicating uncertainty intervals ; Uncertainty in calibration outputs is not reported.")
# 1524 : How.is.uncertainty.in.calibration.outputs.reported. == "Graphical: As a plot; e.g., a line with shaded areas indicating uncertainty intervals ; Uncertainty in calibration outputs is not reported."



Ruchita: 
  # 351 :  How.many.parameters.were.calibrated. is empty
  # 1227: External.beliefs.or.evidence is empty
  # 789: How.are.calibration.results.reported. == "Numerical: e.g. as value(s).; Calibration results are not reported. " - These options are not compatible. It seems to be either an unreported multiple calibration case (not tagged on covidence), or a mistake for extractors (Both Yunfei and I used these options, which did not result in a )
  # 1524 : How.is.uncertainty.in.calibration.outputs.reported. == "Graphical: As a plot; e.g., a line with shaded areas indicating uncertainty intervals ; Uncertainty in calibration outputs is not reported." - Same as above

Nicole: 
  # 3216 : Name.of.calibration.algorithm is set to a non-existing option ("Method of moments"). If you think this is the algorithm used, you should set "Name of algorithm" to "Other", and then set "Other details" below to "Method of moments". 
  # 894 : Name.of.calibration.algorithm is set to a non-existing option ("Latin Hypercube Sampling")	


```

```{r}
raw_data  |> 
  group_by(External.beliefs.or.evidence)|>
  summarise(n())

raw_data |> 
    filter(grepl("Other", External.beliefs.or.evidence))
# 2293 : Other: No prior knowledge is incorporated for parameters to be calibrated.	
# 1279 : Other: No prior knowledge is incorporated for parameters to be calibrated.	


# Correct
raw_data  |> 
  mutate(External.beliefs.or.evidence = ifelse(External.beliefs.or.evidence== "Other: No prior knowledge is incorporated for parameters to be calibrated.", "No prior knowledge is incorporated for parameters to be calibrated. ", External.beliefs.or.evidence))|>
  group_by(External.beliefs.or.evidence)|>
  summarise(n())

```

```{r}
# Raw data outliers
raw_data  |> 
  filter(How.many.calibration.targets.were.used.for.estimation. =="Other: Single")
# 1960 :  How.many.calibration.targets.were.used.for.estimation. is set to "Other: Single"

raw_data  |> 
  group_by(How.many.calibration.targets.were.used.for.estimation.)|>
  summarise(n())

# Correct
raw_data  |> 
  mutate(How.many.calibration.targets.were.used.for.estimation. = ifelse(How.many.calibration.targets.were.used.for.estimation.== "Other: Single", "Single ", How.many.calibration.targets.were.used.for.estimation.))|>
  group_by(How.many.calibration.targets.were.used.for.estimation.)|>
  summarise(n())
```


```{r}
# Raw data outliers
raw_data  |> 
  group_by(Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm)|>
  summarise(n())
# 2366: Other: Other	
# 805: Other: Data likelihood	


# Correct
raw_data  |> 
  mutate(Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm = ifelse(Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm== "Other: Other", "Other ", Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm))|>
    mutate(Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm = ifelse(Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm== "Other: Data likelihood", "Data likelihood ", Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm))|>
  group_by(Goodness.of.fit..GOF..measure.employed.within.calibration.algorithm)|>
  summarise(n())

```


Date cleaning notes: 
* Some variables were set to options that did not exist in the original data extraction form
    * Some were empty -> consensus person mistake 
    * Some were set to non existing options (e.g. "Name of calibration algorithm" set to "Latin hypercube sampling")  -> consensus person mistake 
    * Some we set to existing options but with a type (e.g. a missing space at the end)
    
Notes: 
* Articles with multiple calibration: Each calibration counts as one article
* for "cleaned data table": report both calibrations in a single row ? 
* for "data analysis table": make one row per calibration
* Calculate range of calibration size by algorithm. + remove point estimates



Ideas of plots: 
* Range plot for range of calibration size by algorithm
* Chord diagram to show the association between two categorical variables (e.g. the disease and calibration algorithm, the calibration algorithm and nature of calibration outputs)


To do
* Go check out the outlier articles - Done 
* Download the new data and clean/run the code - done
* Save the IDs of articles with "Multiple calibration" tag
* Apply function for counting options twice to all variables in objective 1


Questions for Emmanuelle: 
* What is considered multiple calibration ? For instance, some authors use one algorithm (GWO) to obtain point estimates first, then MCMC on the GWO outputs to obtain distribution estimates. In theory they are using two different algorithms, but sequencially a single calibration process. In the end, they only use the MCMC outputs to run their model.

